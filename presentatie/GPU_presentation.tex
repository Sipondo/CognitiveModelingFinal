\documentclass{beamer}

\usepackage[british]{babel}
\usepackage{graphicx,hyperref,ru,url}

% Objective of the paper: what is the goal of this work? what problem is addressed? what was the current state of the art? who is the work aimed at?
% Proposal made in the paper: what is the new idea presented? what contributions does the paper claim to make? which one do you consider the most significant?
% Evidence given: how does the paper attempt to support its claims? through theorems/case studies/simulations/benchmarks? does the provided evidence indeed support the claims made?
% Shoulders of giants: what previous research does this work build on? what are the key underlying theoretical ideas? could this work have been done earlier?
% Impact: has this work been influential? what contribution do later papers mainly refer to? is the work still relevant or has it been superceded? is money being made from this idea?
% Discussion points (for presentation): give a number of questions, possibly including those emailed to you by your fellow students, which should arise in the discussion that follows.
% Discussion (for review): summarize the discussion during the meeting.


% The title of the presentation:
%  - first a short version which is visible at the bottom of each slide;
%  - second the full title shown on the title slide;
\title[Deep learning using graphics processors]{
  Review: Large-scale deep unsupervised learning using graphics processors}

% Optional: a subtitle to be dispalyed on the title slide
\subtitle{Rajat Raina, Anand Madhavan, and Andrew Y. Ng}

% The author(s) of the presentation:
%  - again first a short version to be displayed at the bottom;
%  - next the full list of authors, which may include contact information;
\author[Ties Robroek]{
  Ties Roroek \\\medskip
  {\small \url{ties.robroek@student.ru.nl}}}

% The institute:
%  - to start the name of the university as displayed on the top of each slide
%    this can be adjusted such that you can also create a Dutch version
%  - next the institute information as displayed on the title slide
\institute[Radboud University Nijmegen]{
  Institute for Computing and Information Sciences \\
  Radboud University Nijmegen}

% Add a date and possibly the name of the event to the slides
%  - again first a short version to be shown at the bottom of each slide
%  - second the full date and event name for the title slide
\date[March 2018]{
  March 2018}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}
  \frametitle{Outline}

  \tableofcontents
\end{frame}

% Section titles are shown in at the top of the slides with the current section
% highlighted. Note that the number of sections determines the size of the top
% bar, and hence the university name and logo. If you do not add any sections
% they will not be visible.
\section{Introduction}

\begin{frame}
  \frametitle{Introduction}

  \begin{itemize}
    \item Deep belief networks
    \item Sparse coding
    \item Computational constraints
  \end{itemize}
\end{frame}

\section{Timeframe}

\begin{frame}
  \frametitle{Timeframe}

  \begin{itemize}
    \item The paper is from 2009
    \item CUDA was introduced in 2007.
    \item GTX 280, Compute Capability 1.3
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Timeframe}

  \begin{figure}[h]
  \center
  \includegraphics[width=0.7\textwidth]{CUDA}
  {\tiny
  \begin{verbatim}
  Kestener, P. (2015). Introduction to GPU computing with CUDA
  \end{verbatim}
  }
  \end{figure}
\end{frame}


\section{Method}

\begin{frame}
  \frametitle{Deep Belief Networks}

  \begin{itemize}
    \item "Oldschool" multilayer neural network models
    \item Hinton et al.
    \item 2010 State-of-the-art CIFAR-10
  \end{itemize}
  % {\tiny
  % \begin{verbatim}
  % Convolutional Deep Belief Networks on CIFAR-10, Alex Krizhevsky, 2010
  % \end{verbatim}
  % }
\end{frame}

\begin{frame}[fragile]
  \frametitle{Deep Belief Networks}

  \begin{figure}[h]
  \center
  \includegraphics[width=0.7\textwidth]{greedy_deep}
  {\tiny
  \begin{verbatim}
  Hasan H Topcu, 2016
  \end{verbatim}
  }
  \end{figure}
\end{frame}
% Using GPUs for Machine Learning Algorithms
% Eighth International Conference on Document Analysis and Recognition (ICDAR'05) (2005)
% new networks date from 2011
% https://www.cs.toronto.edu/~kriz/conv-cifar10-aug2010.pdf
% state-of-the-art

\begin{frame}
  \frametitle{Spare Coding}

  % \begin{itemize}
    Translate input data to compact representations (building blocks).\\
    Optimize over A and B.
  % \end{itemize}
\end{frame}

\section{Evidence}

\begin{frame} [fragile]
  \frametitle{Evidence}

  \begin{figure}[h]
  \center
  \includegraphics[width=0.7\textwidth]{table2}
  {\tiny
  \begin{verbatim}
  Google 2018
  \end{verbatim}
  }
  \end{figure}
  % \begin{block}{Slides with \LaTeX}
  %   Beamer offers a lot of functions to create nice slides using \LaTeX.
  % \end{block}
  %
  % \begin{block}{The basis}
  %   This style uses the following default styles:
  %   \begin{itemize}
  %     \item split
  %     \item whale
  %     \item rounded
  %     \item orchid
  %   \end{itemize}
  % \end{block}
\end{frame}

\begin{frame} [fragile]
  \frametitle{Evidence}

  \begin{figure}[h]
  \center
  \includegraphics[width=0.7\textwidth]{table3}
  {\tiny
  \begin{verbatim}
  Google 2018
  \end{verbatim}
  }
  \end{figure}
\end{frame}

\begin{frame} [fragile]
  \frametitle{Evidence}

  \begin{figure}[h]
  \center
  \includegraphics[width=0.7\textwidth]{table4}
  {\tiny
  \begin{verbatim}
  Google 2018
  \end{verbatim}
  }
  \end{figure}
\end{frame}


\section{Impact}

\begin{frame}
  \frametitle{Impact}

  \begin{enumerate}
    \item This has just been the beginning for CUDA in ML.
    \item State-of-the-art methods are highly dependent on this.
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Impact}

  \begin{figure}[h]
  \center
  \includegraphics[width=0.7\textwidth]{deeplearning}
  {\tiny
  \begin{verbatim}
  Google 2018
  \end{verbatim}
  }
  \end{figure}
\end{frame}

\section{Conclusion}

\begin{frame}
  \frametitle{Conclusion}

  \begin{itemize}
    \item Research is simple but effective
    \item Paved the way for GPU's in Machine Learning
  \end{itemize}
\end{frame}


\section{Discussion}

\begin{frame}
  \frametitle{Discussion}

  \begin{enumerate}
    \item Is the comparison between CPU and GPU fair?
    \item Are the techniques still state-of-the-art?
    \item What are the disadvantages of the GPU for machine learning tasks?
    % \item The authors say that they chose these two methods for GPU implementation because they are suitable for problems with high dimensional inputs. DBNs are RBM based networks, but could an autoencoder network not have a similar advantage? Can you infer the reasoning of choosing a DBN and not autoencoder?
    \item DBNs are RBM based networks, but could they have also made networks with autoencoders?
    % \item What, if there is any, is the actual difference between the Deep Belief Networks and Neural Networks? Or are Neural Networks merely possible due to the coming of the GPUs?
    % https://en.wikipedia.org/wiki/Deep_belief_network
    % The pretraining of DBN has shown to be beneficial if the training set is small.
    % \item What do you think is the reason for the different speed up magnitudes? (Table 2 vs Table 3 and 4)
    \item Is the comparison between chosen GPU and CPU’s a fair one? Why did they choose to compare these?
    \item - What types of algorithms are suitable to use a GPU for?
    % \item - Is there a mathematical description of the constraints posed by the usage of a GPU? This would make designing of the “parallel” algorithms easier.
    % \item It seems that the CPU and GPU implementations differ in ways irrelevant to the capabilities of GPUs and CPUs. If I understand correctly, implementing an algorithm in CUDA is very much a low-level endeavor. By contrast, one CPU implementation was written in Matlab, while the other used a third-party linear algebra library. Could this explain (or mask) some of the difference in performance?
    \item Do you think there is a reason the authors chose DBNs/sparse coding over some `more wellknown' unsupervised learning methods like clustering? If so, what reason?
    % \item A problem that still remains is that information transfer to the GPU takes relatively long. In the paper they argue that "A partial solution is to perform memory transfers only in large batches, grouped over several computations.". Do you think this problem can be solved? Or do you think this problem will vanish if we keep the number of transfers as small as possible?
    % \item Why did they do their experiments with unsupervised learning? Would there be a different effect if they use supervised learning or reinforcement learning?
    % \item Does this mean we can just translate something to a linear algebra problem and expect the same huge improvements?
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Discussion}

  \begin{figure}[h]
  \center
  \includegraphics[width=0.7\textwidth]{Fermi}
  {\tiny
  \begin{verbatim}
  Nvidia 2010
  \end{verbatim}
  }
  \end{figure}
\end{frame}



\end{document}
